*** cli.py	orig
--- cli.py	patch
@@
-import argparse
-import os, json
-from .parser.loader import load_grammar_text
-from .parser.transform import HasslTransformer
-from .ast.nodes import Program
-from lark import Lark
-from .semantics.analyzer import analyze
-from .codegen.package import emit_package
-from .codegen import generate as codegen_generate
+import argparse
+import os, json, glob
+from pathlib import Path
+from typing import Dict, Tuple, List
+from .parser.loader import load_grammar_text
+from .parser.transform import HasslTransformer
+from .ast.nodes import Program, Alias, Schedule
+from lark import Lark
+from .semantics import analyzer as sem_analyzer
+from .semantics.analyzer import analyze
+from .codegen.package import emit_package
+from .codegen import generate as codegen_generate
 
 #GRAMMAR_PATH = os.path.join(os.path.dirname(__file__), "parser", "hassl.lark")
 
-def parse_hassl(text: str) -> Program:
+def parse_hassl(text: str) -> Program:
     grammar = load_grammar_text()
     parser = Lark(grammar, start="start", parser="lalr", maybe_placeholders=False)
     tree = parser.parse(text)
     program = HasslTransformer().transform(tree)
     return program
 
+def _derive_package_name(prog: Program, src_path: Path, module_root: Path | None) -> str:
+    """
+    If the source did not declare `package`, derive one from the path:
+    - If module_root is given and src_path is under it: use relative path (dots)
+    - Else: use file stem
+    """
+    if getattr(prog, "package", None):
+        return prog.package  # declared
+    if module_root:
+        try:
+            rel = src_path.resolve().relative_to(module_root.resolve())
+            parts = list(rel.with_suffix("").parts)
+            if parts:
+                return ".".join(parts)
+        except Exception:
+            pass
+    return src_path.stem
+
+def _collect_public_exports(prog: Program, pkg: str) -> Dict[Tuple[str,str,str], object]:
+    """
+    Build (pkg, kind, name) -> node for public alias/schedule in a single Program.
+    Accepts both Schedule nodes and transformer dicts {"type":"schedule_decl",...}.
+    """
+    out: Dict[Tuple[str,str,str], object] = {}
+    # Aliases
+    for s in prog.statements:
+        if isinstance(s, Alias):
+            if not getattr(s, "private", False):
+                out[(pkg, "alias", s.name)] = s
+    # Schedules (either dicts from transformer or Schedule nodes)
+    for s in prog.statements:
+        if isinstance(s, Schedule):
+            if not getattr(s, "private", False):
+                out[(pkg, "schedule", s.name)] = s
+        elif isinstance(s, dict) and s.get("type") == "schedule_decl" and not s.get("private", False):
+            name = s.get("name")
+            if isinstance(name, str) and name.strip():
+                # Wrap a lightweight Schedule node for uniformity with analyzer
+                out[(pkg, "schedule", name)] = Schedule(name=name, clauses=s.get("clauses", []) or [], private=False)
+    return out
+
+def _scan_hassl_files(path: Path) -> List[Path]:
+    if path.is_file():
+        return [path]
+    # directory: recurse for **/*.hassl
+    return [Path(p) for p in glob.glob(str(path / "**" / "*.hassl"), recursive=True)]
+
 def main():
     ap = argparse.ArgumentParser(prog="hasslc", description="HASSL Compiler")
-    ap.add_argument("input", help="Input .hassl file")
-    ap.add_argument("-o", "--out", default="./packages/out", help="Output directory for HA package")
+    ap.add_argument("input", help="Input .hassl file OR directory")
+    ap.add_argument("-o", "--out", default="./packages/out", help="Output directory root for HA package(s)")
+    ap.add_argument("--module-root", default=None, help="Optional root to derive package names from paths")
     args = ap.parse_args()
 
-    with open(args.input) as f:
-        src = f.read()
+    in_path = Path(args.input)
+    out_root = Path(args.out)
+    module_root = Path(args.module_root).resolve() if args.module_root else None
+
+    src_files = _scan_hassl_files(in_path)
+    if not src_files:
+        raise SystemExit(f"[hasslc] No .hassl files found in {in_path}")
 
-    program = parse_hassl(src)
-    print("[hasslc] AST:", program.to_dict())
-    ir = analyze(program)
-    print("[hasslc] IR:", ir.to_dict())
+    # --- Pass 0: parse all and assign/derive package names
+    programs: List[tuple[Path, Program, str]] = []
+    for p in src_files:
+        with open(p, "r", encoding="utf-8") as f:
+            text = f.read()
+        prog = parse_hassl(text)
+        pkg_name = _derive_package_name(prog, p, module_root)
+        # persist the derived name back into Program for the analyzer
+        try:
+            prog.package = pkg_name
+        except Exception:
+            # If Program is frozen/legacy, we still carry pkg_name in tuple
+            pass
+        programs.append((p, prog, pkg_name))
+
+    # --- Pass 1: collect public exports across all programs
+    GLOBAL_EXPORTS: Dict[Tuple[str,str,str], object] = {}
+    for path, prog, pkg in programs:
+        GLOBAL_EXPORTS.update(_collect_public_exports(prog, pkg))
+
+    # publish global exports to analyzer
+    sem_analyzer.GLOBAL_EXPORTS = GLOBAL_EXPORTS
 
-    ir_dict = ir.to_dict() if hasattr(ir, "to_dict") else ir
-    codegen_generate(ir_dict, args.out)
-    print(f"[hasslc] Package written to {args.out}")
+    # --- Pass 2: analyze each program with global view
+    os.makedirs(out_root, exist_ok=True)
+    all_ir = []
+    for path, prog, pkg in programs:
+        print(f"[hasslc] Parsing {path}  (package: {pkg})")
+        print("[hasslc] AST:", json.dumps(prog.to_dict(), indent=2))
+        ir = analyze(prog)
+        print("[hasslc] IR:", json.dumps(ir.to_dict(), indent=2))
+        all_ir.append((pkg, ir))
 
-    os.makedirs(args.out, exist_ok=True)
-    emit_package(ir, args.out)
-    with open(os.path.join(args.out, "DEBUG_ir.json"), "w") as dbg:
-        dbg.write(json.dumps(ir.to_dict(), indent=2))
-    print(f"[hasslc] Package written to {args.out}")
+    # --- Emit: per package subdir
+    for pkg, ir in all_ir:
+        pkg_dir = out_root / pkg.replace(".", os.sep)
+        os.makedirs(pkg_dir, exist_ok=True)
+        ir_dict = ir.to_dict() if hasattr(ir, "to_dict") else ir
+        # If your codegen emits a whole HA package, keep both calls;
+        # some repos use either emit_package or codegen_generate â€” we run both defensively.
+        codegen_generate(ir_dict, str(pkg_dir))
+        emit_package(ir, str(pkg_dir))
+        with open(pkg_dir / "DEBUG_ir.json", "w", encoding="utf-8") as dbg:
+            dbg.write(json.dumps(ir.to_dict(), indent=2))
+        print(f"[hasslc] Package written to {pkg_dir}")
+
+    # Also drop a cross-project export table for debugging
+    with open(out_root / "DEBUG_exports.json", "w", encoding="utf-8") as fp:
+        # make keys printable
+        printable = {f"{k[0]}::{k[1]}::{k[2]}": ("Alias" if isinstance(v, Alias) else "Schedule") for k, v in GLOBAL_EXPORTS.items()}
+        json.dump(printable, fp, indent=2)
+    print(f"[hasslc] Global exports index written to {out_root / 'DEBUG_exports.json'}")
 
 if __name__ == "__main__":
     main()